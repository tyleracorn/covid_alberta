{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# webscraper: albertaC19\n",
    "\n",
    "> The module that grabs updated covid-19 data from the alberta  [Covid19stats](https://covid19stats.alberta.ca/) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class albertaC19():\n",
    "    def __init__(self, covid_url:str='https://covid19stats.alberta.ca/', outputfolder:str='data',\n",
    "                 html_update_ids:dict=None, totals_update_fig_order:dict=None):\n",
    "        '''\n",
    "        using requests and BeautfulSoup4 scrape updated covid data from the ablerta website\n",
    "        save the outputs into a outputfolder\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            covid_url:str\n",
    "                the url for the alberta covid website\n",
    "            outputfolder:str\n",
    "                the folder to save the scraped data to. Will create the folder if it can't find it\n",
    "            html_update_ids:dict\n",
    "                if the alberta covid html changes the html id where the specified data is stored you\n",
    "                can update to the new id here. This is used in the scrapers. current keys are\n",
    "                `totals`, `regions`, `testing`\n",
    "        '''\n",
    "        self.covid_url = covid_url\n",
    "        self.outputfolder = Path(outputfolder)\n",
    "        if not self.outputfolder.is_dir(): self.outputfolder.mkdir()\n",
    "\n",
    "        self.page = requests.get(self.covid_url)\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        self.html_ids = {'totals':'cases', 'regions':'geospatial', 'testing': 'laboratory-testing'}\n",
    "        if html_update_ids:\n",
    "            self.html_ids.update(html_update_ids)\n",
    "        self.totals_figure_order = {'cum_cases':0, 'daily_cases':3, 'case_status':1}\n",
    "        if totals_update_fig_order:\n",
    "            self.totals_figure_order.update(totals_update_fig_order)\n",
    "\n",
    "    def print_html_class_ids(self, html_class_attr:str='level2', print_self:bool=True):\n",
    "        '''\n",
    "        websites change so use this if you need to figure out what to use to update the `html_update_class_ids`\n",
    "        dictionary. Right now the covid alberta site is using `level 2` as one of the id attributes for\n",
    "        the tabs containing the covid data. The scrapers in this are searching for the `id` string.\n",
    "\n",
    "        this function will find all `div` `class` sections in the soup with\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            html_class_attr:str\n",
    "                will search through the soup find all class names that include this attribute string\n",
    "            print_self:bool\n",
    "                will also print the current python class parameters `html_class_ids` which are used\n",
    "                by the scraper functions to find the data\n",
    "\n",
    "        '''\n",
    "        find_results = False\n",
    "        for tag in self.soup.find_all('div', {'class': html_class_attr}):\n",
    "            if tag.attrs: find_results = True\n",
    "            print(tag.attrs)\n",
    "        if not find_results:\n",
    "            print(f'unable to find tag: {html_class_attr}')\n",
    "        if print_self:\n",
    "            print(\"Here are the class id's we are currently using for this scraper:\")\n",
    "            print(self.html_ids)\n",
    "        return find_results\n",
    "\n",
    "    def update_html_ids(self, html_update_ids:dict=None):\n",
    "        '''\n",
    "        update the ids used to search for the data in the alberta covid html\n",
    "        current keys are `totals`, `regions`, `testing`\n",
    "        '''\n",
    "        if html_update_ids:\n",
    "            self.html_ids.update(html_update_ids)\n",
    "            \n",
    "    def update_fig_order(self, totals_update_fig_order:dict=None):\n",
    "        '''\n",
    "        The order the figures are displayed on the website using python 0 index. This is important\n",
    "        because the associated tags use a randomly generated id name so I am accessing them by order\n",
    "        `self.totals_figure_order` will print out the default order expected\n",
    "        '''\n",
    "        if totals_update_fig_order:\n",
    "            self.totals_figure_order.update(totals_update_fig_order)\n",
    "\n",
    "    def _clean_cumulative_data(self, ab_cumulative_dict:dict):\n",
    "        '''\n",
    "        utility function used to clean up the cumulative data\n",
    "        '''\n",
    "        dates = ab_cumulative_dict['x']['data'][0]['x']\n",
    "        cum_cases = ab_cumulative_dict['x']['data'][0]['y']\n",
    "        # Convert to DataFrame\n",
    "        df_ab_cumulative = pd.DataFrame(data=cum_cases,\n",
    "                                        index=dates,\n",
    "                                        columns=['cum_cases'])\n",
    "        # Clean up formatting of df\n",
    "        df_ab_cumulative.index = pd.to_datetime(df_ab_cumulative.index)\n",
    "\n",
    "        return df_ab_cumulative\n",
    "\n",
    "    def _clean_daily_case_data(self, ab_daily_cases:dict):\n",
    "        '''\n",
    "        utility function used to clean up the daily case data\n",
    "        '''\n",
    "        daily_data = dict()\n",
    "        for data in ab_daily_cases['x']['data']:\n",
    "            daily_data[data['name']] = {'date': data['x'], '{0}_count'.format(data['name']): data['y']}\n",
    "        if len(daily_data) != 2:\n",
    "            raise Warning(\"expecting only 2 daily case categories. Website likely changed. Check the results\")\n",
    "        df_confirmed = pd.DataFrame(data=daily_data['Confirmed']['Confirmed_count'],\n",
    "                                    index=daily_data['Confirmed']['date'],\n",
    "                                    columns=['Confirmed_count'])\n",
    "        df_confirmed.index = pd.to_datetime(df_confirmed.index)\n",
    "        df_probable = pd.DataFrame(data=daily_data['Probable']['Probable_count'],\n",
    "                                   index=daily_data['Probable']['date'],\n",
    "                                   columns=['Probable_count'])\n",
    "        df_probable.index = pd.to_datetime(df_probable.index)\n",
    "\n",
    "        df_daily_cases = df_confirmed.join(df_probable)\n",
    "        df_daily_cases['Daily_count'] = df_daily_cases.sum(axis=1)\n",
    "\n",
    "        return df_daily_cases\n",
    "\n",
    "    def _clean_case_status_data(self, ab_case_status:dict):\n",
    "        '''\n",
    "        utility function used to clean up the case status data\n",
    "        '''\n",
    "        status_data = dict()\n",
    "        for data in ab_case_status['x']['data']:\n",
    "            status_data[data['name']] = {'date': data['x'], '{0}_cum'.format(data['name']): data['y']}\n",
    "        if len(status_data) != 3:\n",
    "            raise Warning(\"expecting only 3 status case categories. Website likely changed. Check the results\")\n",
    "\n",
    "        df_active = pd.DataFrame(data=status_data['Active']['Active_cum'],\n",
    "                                 index=status_data['Active']['date'],\n",
    "                                 columns=['Active_cum'])\n",
    "        df_active.index = pd.to_datetime(df_active.index)\n",
    "        df_died = pd.DataFrame(data=status_data['Died']['Died_cum'],\n",
    "                               index=status_data['Died']['date'],\n",
    "                               columns=['Died_cum'])\n",
    "        df_died.index = pd.to_datetime(df_died.index)\n",
    "        df_recovered = pd.DataFrame(data=status_data['Recovered']['Recovered_cum'],\n",
    "                                    index=status_data['Recovered']['date'],\n",
    "                                    columns=['Recovered_cum'])\n",
    "        df_recovered.index = pd.to_datetime(df_recovered.index)\n",
    "\n",
    "        df_case_status = df_active.join([df_died, df_recovered])\n",
    "\n",
    "        return df_case_status\n",
    "\n",
    "    def scrape_albertaTotals(self, output_filename:str='alberta_total_data', fltypes=('csv', 'json'),\n",
    "                             update_figure_order=None, return_dataframe:bool=False):\n",
    "        '''scrape the total case counts in alberta and save the data to the output folder\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            output_filename:str\n",
    "                filename without the file ending\n",
    "            fltypes:[list or str]\n",
    "                will save out either csv, json or both filetypes. use `None` to not write anything\n",
    "            update_figure_order:dict\n",
    "                the order the figures are displayed on the website using python 0 index. This is important\n",
    "                because the associated tags use a randomly generated id name so I am accessing them by order\n",
    "                `self.totals_figure_order` will print out the default order expected\n",
    "            return_dataframe:bool\n",
    "                will return either the dataframes or a true/false on write success\n",
    "        '''\n",
    "        results = self.soup.find(id=self.html_ids['totals'])\n",
    "        totals_results = results.find_all('script')\n",
    "        fig_order = self.totals_figure_order.copy()\n",
    "        if update_figure_order:\n",
    "            fig_order.update(update_figure_order)\n",
    "\n",
    "        # Scrape the data\n",
    "        ab_cumulative = json.loads(totals_results[fig_order['cum_cases']].string)\n",
    "        ab_daily_cases = json.loads(totals_results[fig_order['daily_cases']].string)\n",
    "        ab_case_status = json.loads(totals_results[fig_order['case_status']].string)\n",
    "\n",
    "        df_ab_cumulative = self._clean_cumulative_data(ab_cumulative)\n",
    "        df_ab_daily_cases = self._clean_daily_case_data(ab_daily_cases)\n",
    "        df_ab_case_status = self._clean_case_status_data(ab_case_status)\n",
    "\n",
    "        df_ab_all = df_ab_cumulative.join([df_ab_daily_cases, df_ab_case_status])\n",
    "        # clean up df formatting\n",
    "        df_ab_all.fillna(0, inplace=True)\n",
    "        df_ab_all = df_ab_all.astype(int)\n",
    "\n",
    "        # Write out the data. If fltypes = None the function will return False\n",
    "        write_success = self._write_dataframe(df_ab_all, output_filename, fltypes)\n",
    "        if return_dataframe:\n",
    "            return df_ab_all\n",
    "        return write_success\n",
    "\n",
    "    def scrape_albertaRegions(self, output_filename:str='alberta_region_data', fltypes=('csv', 'json'),\n",
    "                              return_dataframe:bool=False):\n",
    "        '''scrape the total case counts in alberta by region and save the data\n",
    "        to the output folder\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            output_filename:str\n",
    "                filename without the file ending\n",
    "            fltypes:[list or str]\n",
    "                will save out either csv, json or both filetypes\n",
    "            return_dataframe:bool\n",
    "                will return either the dataframes or a true/false on write success\n",
    "        '''\n",
    "        results = self.soup.find(id=self.html_ids['regions'])\n",
    "\n",
    "        region_results = results.find_all('script')\n",
    "        results_as_dict = json.loads(region_results[0].string)['x']\n",
    "\n",
    "        zone_len = len(results_as_dict['data'])\n",
    "        region_data_dict = dict()\n",
    "        for idx in range (zone_len):\n",
    "            zone_name = results_as_dict['data'][idx]['name']\n",
    "            region_data_dict[zone_name] = {'date':results_as_dict['data'][idx]['x'],\n",
    "                                           'cumulative':results_as_dict['data'][idx]['y']}\n",
    "\n",
    "        list_ab_regions = list()\n",
    "        for idx, key in enumerate(region_data_dict.keys()):\n",
    "            if 'Zone' in key:\n",
    "                zone = key.strip(' Zone')\n",
    "            else:\n",
    "                zone = key\n",
    "            column = f'{zone}_cumulative'\n",
    "            list_ab_regions.append(pd.DataFrame(data=region_data_dict[key]['cumulative'],\n",
    "                                                index=region_data_dict[key]['date'],\n",
    "                                                columns=[column]))\n",
    "\n",
    "        df_ab_regions = list_ab_regions[0].join(list_ab_regions[1:])\n",
    "        df_ab_regions.index = pd.to_datetime(df_ab_regions.index)\n",
    "        df_ab_regions.fillna(0, inplace=True)\n",
    "        df_ab_regions = df_ab_regions.astype(int)\n",
    "\n",
    "        # Write out the data. If fltypes = None the function will return False\n",
    "        write_success = self._write_dataframe(df_ab_regions, output_filename, fltypes)\n",
    "        if return_dataframe:\n",
    "            return df_ab_regions\n",
    "        return write_success\n",
    "\n",
    "    def scrape_albertaTesting(self, output_filename:str='alberta_testing_data', fltypes=('csv', 'json'),\n",
    "                              return_dataframe:bool=False):\n",
    "        '''scrape the testing counts by date in alberta and save the data\n",
    "        to the output folder\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            output_filename:str\n",
    "                filename without the file ending\n",
    "            fltypes:[list or str]\n",
    "                will save out either csv, json or both filetypes\n",
    "            return_dataframe:bool\n",
    "                will return either the dataframes or a true/false on write success\n",
    "        '''\n",
    "        results = self.soup.find(id=self.html_ids['testing'])\n",
    "        testing_results = results.find_all('script')\n",
    "        if len(testing_results) != 1:\n",
    "            raise Warning(\"expecting only 1 test case categories. Website likely changed. Check the results\")\n",
    "        # Scrape the data\n",
    "        tests_as_dict = json.loads(testing_results[0].string)['x']\n",
    "        dates = tests_as_dict['data'][0]['x']\n",
    "        test_count = tests_as_dict['data'][0]['y']\n",
    "        # Convert to DataFrame\n",
    "        df_ab_tests = pd.DataFrame(data=test_count,\n",
    "                                        index=dates,\n",
    "                                        columns=['test_count'])\n",
    "        df_ab_tests.index = pd.to_datetime(df_ab_tests.index)\n",
    "        df_ab_tests = df_ab_tests.astype(int)\n",
    "\n",
    "        # Write out the data. If fltypes = None the function will return False\n",
    "        write_success = self._write_dataframe(df_ab_tests, output_filename, fltypes)\n",
    "        if return_dataframe:\n",
    "            return df_ab_tests\n",
    "        return write_success\n",
    "\n",
    "    def _write_dataframe(self, dataframe:pd.DataFrame, output_filename:str, fltypes):\n",
    "        ''''\n",
    "        utility function to write the dataframe scraped. This way we can easily add different\n",
    "        write functions to all the scraping functions.\n",
    "\n",
    "        Parameters:\n",
    "        \n",
    "            dataframe:pd.DataFrame\n",
    "                pd.DataFrame to write out\n",
    "            output_filename:str\n",
    "                filename without the file ending\n",
    "            fltypes:[list or str]\n",
    "                will save out either csv, json or both filetypes\n",
    "        \n",
    "        ----\n",
    "        Returns:\n",
    "        \n",
    "            write_success:bool\n",
    "                whether it wrote anything out or not. If `None` for fltypes is passed\n",
    "                will return `False`\n",
    "\n",
    "        '''\n",
    "        write_success = False\n",
    "        # Write out the data\n",
    "        if fltypes:\n",
    "            if 'json' in fltypes:\n",
    "                flpath = self.outputfolder.joinpath(output_filename).with_suffix('.json')\n",
    "                dataframe.to_json(flpath)\n",
    "                write_success = True\n",
    "            if 'csv' in fltypes:\n",
    "                flpath = self.outputfolder.joinpath(output_filename).with_suffix('.csv')\n",
    "                dataframe.to_csv(flpath)\n",
    "                write_success = True\n",
    "        return write_success\n",
    "\n",
    "    def scrape_all(self, totalfl:str='alberta_total_data', regionsfl:str='alberta_region_data',\n",
    "                   testfl:str='alberta_testing_data', fltypes=('csv', 'json'),\n",
    "                   combine_dataframes:bool=False, return_dataframes:bool=False):\n",
    "        '''scrape the total alberta covid-19 case counts, the covid-19 case counts by\n",
    "        region and the testing data from the alberta covid-19 website\n",
    "\n",
    "        Parameters:\n",
    "            output_filename:str\n",
    "                filename without the file ending\n",
    "            fltypes:[list or str]\n",
    "                will save out either csv, json or both filetypes\n",
    "            return_dataframe:bool\n",
    "                will return either the dataframes or a true/false on write success\n",
    "        ----\n",
    "        Returns:\n",
    "        \n",
    "            all_data\n",
    "                if combine_dataframes = True\n",
    "            totals, regions, testing\n",
    "                if combine_dataframes = False\n",
    "\n",
    "        '''\n",
    "        totals = self.scrape_albertaTotals(output_filename=totalfl, fltypes=fltypes, return_dataframe=return_dataframes)\n",
    "        regions = self.scrape_albertaRegions(output_filename=regionsfl, fltypes=fltypes, return_dataframe=return_dataframes)\n",
    "        testing = self.scrape_albertaTesting(output_filename=testfl, fltypes=fltypes, return_dataframe=return_dataframes)\n",
    "        if combine_dataframes:\n",
    "            all_data = totals.join([regions, testing])\n",
    "            return all_data\n",
    "        return totals, regions, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.__init__\" class=\"doc_header\"><code>albertaC19.__init__</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.__init__</code>(**`covid_url`**:`str`=*`'https://covid19stats.alberta.ca/'`*, **`outputfolder`**:`str`=*`'data'`*, **`html_update_ids`**:`dict`=*`None`*, **`totals_update_fig_order`**:`dict`=*`None`*)\n",
       "\n",
       "using requests and BeautfulSoup4 scrape updated covid data from the ablerta website\n",
       "save the outputs into a outputfolder\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    covid_url:str\n",
       "        the url for the alberta covid website\n",
       "    outputfolder:str\n",
       "        the folder to save the scraped data to. Will create the folder if it can't find it\n",
       "    html_update_ids:dict\n",
       "        if the alberta covid html changes the html id where the specified data is stored you\n",
       "        can update to the new id here. This is used in the scrapers. current keys are\n",
       "        `totals`, `regions`, `testing`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## albertaC19 scraping methods\n",
    "\n",
    "The main scrapping methods are shown here along with some examples of what the outputs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.scrape_albertaTotals\" class=\"doc_header\"><code>albertaC19.scrape_albertaTotals</code><a href=\"__main__.py#L143\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.scrape_albertaTotals</code>(**`output_filename`**:`str`=*`'alberta_total_data'`*, **`fltypes`**=*`('csv', 'json')`*, **`update_figure_order`**=*`None`*, **`return_dataframe`**:`bool`=*`False`*)\n",
       "\n",
       "scrape the total case counts in alberta and save the data to the output folder\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    output_filename:str\n",
       "        filename without the file ending\n",
       "    fltypes:[list or str]\n",
       "        will save out either csv, json or both filetypes. use `None` to not write anything\n",
       "    update_figure_order:dict\n",
       "        the order the figures are displayed on the website using python 0 index. This is important\n",
       "        because the associated tags use a randomly generated id name so I am accessing them by order\n",
       "        `self.totals_figure_order` will print out the default order expected\n",
       "    return_dataframe:bool\n",
       "        will return either the dataframes or a true/false on write success"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.scrape_albertaTotals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the webscraper\n",
    "import covid_alberta\n",
    "abscraper = covid_alberta.albertaC19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cum_cases</th>\n",
       "      <th>Confirmed_count</th>\n",
       "      <th>Probable_count</th>\n",
       "      <th>Daily_count</th>\n",
       "      <th>Active_cum</th>\n",
       "      <th>Died_cum</th>\n",
       "      <th>Recovered_cum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>1250</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>618</td>\n",
       "      <td>23</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>1308</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>676</td>\n",
       "      <td>24</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>1344</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>712</td>\n",
       "      <td>27</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>1409</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>776</td>\n",
       "      <td>27</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>1423</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>876</td>\n",
       "      <td>29</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cum_cases  Confirmed_count  Probable_count  Daily_count  \\\n",
       "2020-04-04       1250               38              19           57   \n",
       "2020-04-05       1308               35              23           58   \n",
       "2020-04-06       1344               20              16           36   \n",
       "2020-04-07       1409               39              26           65   \n",
       "2020-04-08       1423                9               5           14   \n",
       "\n",
       "            Active_cum  Died_cum  Recovered_cum  \n",
       "2020-04-04         618        23            322  \n",
       "2020-04-05         676        24            382  \n",
       "2020-04-06         712        27            449  \n",
       "2020-04-07         776        27            518  \n",
       "2020-04-08         876        29            518  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab totals and print the tail of the dataframe\n",
    "ab_totals = abscraper.scrape_albertaTotals(fltypes=None, return_dataframe=True)\n",
    "ab_totals.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.scrape_albertaRegions\" class=\"doc_header\"><code>albertaC19.scrape_albertaRegions</code><a href=\"__main__.py#L186\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.scrape_albertaRegions</code>(**`output_filename`**:`str`=*`'alberta_region_data'`*, **`fltypes`**=*`('csv', 'json')`*, **`return_dataframe`**:`bool`=*`False`*)\n",
       "\n",
       "scrape the total case counts in alberta by region and save the data\n",
       "to the output folder\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    output_filename:str\n",
       "        filename without the file ending\n",
       "    fltypes:[list or str]\n",
       "        will save out either csv, json or both filetypes\n",
       "    return_dataframe:bool\n",
       "        will return either the dataframes or a true/false on write success"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.scrape_albertaRegions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calgary_cumulative</th>\n",
       "      <th>Central_cumulative</th>\n",
       "      <th>Edmont_cumulative</th>\n",
       "      <th>North_cumulative</th>\n",
       "      <th>South_cumulative</th>\n",
       "      <th>Unknown_cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>778</td>\n",
       "      <td>61</td>\n",
       "      <td>315</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>801</td>\n",
       "      <td>65</td>\n",
       "      <td>340</td>\n",
       "      <td>79</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>821</td>\n",
       "      <td>65</td>\n",
       "      <td>348</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>854</td>\n",
       "      <td>72</td>\n",
       "      <td>364</td>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>860</td>\n",
       "      <td>72</td>\n",
       "      <td>368</td>\n",
       "      <td>95</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Calgary_cumulative  Central_cumulative  Edmont_cumulative  \\\n",
       "2020-04-04                 778                  61                315   \n",
       "2020-04-05                 801                  65                340   \n",
       "2020-04-06                 821                  65                348   \n",
       "2020-04-07                 854                  72                364   \n",
       "2020-04-08                 860                  72                368   \n",
       "\n",
       "            North_cumulative  South_cumulative  Unknown_cumulative  \n",
       "2020-04-04                75                19                   2  \n",
       "2020-04-05                79                21                   2  \n",
       "2020-04-06                86                22                   2  \n",
       "2020-04-07                94                23                   2  \n",
       "2020-04-08                95                26                   2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab regions and print the tail of the dataframe\n",
    "ab_regions = abscraper.scrape_albertaRegions(fltypes=None, return_dataframe=True)\n",
    "ab_regions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.scrape_albertaTesting\" class=\"doc_header\"><code>albertaC19.scrape_albertaTesting</code><a href=\"__main__.py#L234\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.scrape_albertaTesting</code>(**`output_filename`**:`str`=*`'alberta_testing_data'`*, **`fltypes`**=*`('csv', 'json')`*, **`return_dataframe`**:`bool`=*`False`*)\n",
       "\n",
       "scrape the testing counts by date in alberta and save the data\n",
       "to the output folder\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    output_filename:str\n",
       "        filename without the file ending\n",
       "    fltypes:[list or str]\n",
       "        will save out either csv, json or both filetypes\n",
       "    return_dataframe:bool\n",
       "        will return either the dataframes or a true/false on write success"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.scrape_albertaTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calgary_cumulative</th>\n",
       "      <th>Central_cumulative</th>\n",
       "      <th>Edmont_cumulative</th>\n",
       "      <th>North_cumulative</th>\n",
       "      <th>South_cumulative</th>\n",
       "      <th>Unknown_cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>778</td>\n",
       "      <td>61</td>\n",
       "      <td>315</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>801</td>\n",
       "      <td>65</td>\n",
       "      <td>340</td>\n",
       "      <td>79</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>821</td>\n",
       "      <td>65</td>\n",
       "      <td>348</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>854</td>\n",
       "      <td>72</td>\n",
       "      <td>364</td>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>860</td>\n",
       "      <td>72</td>\n",
       "      <td>368</td>\n",
       "      <td>95</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Calgary_cumulative  Central_cumulative  Edmont_cumulative  \\\n",
       "2020-04-04                 778                  61                315   \n",
       "2020-04-05                 801                  65                340   \n",
       "2020-04-06                 821                  65                348   \n",
       "2020-04-07                 854                  72                364   \n",
       "2020-04-08                 860                  72                368   \n",
       "\n",
       "            North_cumulative  South_cumulative  Unknown_cumulative  \n",
       "2020-04-04                75                19                   2  \n",
       "2020-04-05                79                21                   2  \n",
       "2020-04-06                86                22                   2  \n",
       "2020-04-07                94                23                   2  \n",
       "2020-04-08                95                26                   2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab testing data and print the tail of the dataframe\n",
    "ab_testing = abscraper.scrape_albertaRegions(fltypes=None, return_dataframe=True)\n",
    "ab_testing.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.scrape_all\" class=\"doc_header\"><code>albertaC19.scrape_all</code><a href=\"__main__.py#L304\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.scrape_all</code>(**`totalfl`**:`str`=*`'alberta_total_data'`*, **`regionsfl`**:`str`=*`'alberta_region_data'`*, **`testfl`**:`str`=*`'alberta_testing_data'`*, **`fltypes`**=*`('csv', 'json')`*, **`combine_dataframes`**:`bool`=*`False`*, **`return_dataframes`**:`bool`=*`False`*)\n",
       "\n",
       "scrape the total alberta covid-19 case counts, the covid-19 case counts by\n",
       "region and the testing data from the alberta covid-19 website\n",
       "\n",
       "Parameters:\n",
       "    output_filename:str\n",
       "        filename without the file ending\n",
       "    fltypes:[list or str]\n",
       "        will save out either csv, json or both filetypes\n",
       "    return_dataframe:bool\n",
       "        will return either the dataframes or a true/false on write success\n",
       "----\n",
       "Returns:\n",
       "\n",
       "    all_data\n",
       "        if combine_dataframes = True\n",
       "    totals, regions, testing\n",
       "        if combine_dataframes = False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.scrape_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Totals DataFrame\n",
      "['cum_cases', 'Confirmed_count', 'Probable_count', 'Daily_count', 'Active_cum', 'Died_cum', 'Recovered_cum'] \n",
      "\n",
      "Columns in Regions DataFrame\n",
      "['Calgary_cumulative', 'Central_cumulative', 'Edmont_cumulative', 'North_cumulative', 'South_cumulative', 'Unknown_cumulative'] \n",
      "\n",
      "Columns in Testing DataFrame\n",
      "['test_count']\n"
     ]
    }
   ],
   "source": [
    "ab_totals, ab_regions, ab_testing = abscraper.scrape_all(fltypes=None, return_dataframes=True)\n",
    "print('Columns in Totals DataFrame')\n",
    "print(ab_totals.columns.to_list(), '\\n')\n",
    "print('Columns in Regions DataFrame')\n",
    "print(ab_regions.columns.to_list(), '\\n')\n",
    "print('Columns in Testing DataFrame')\n",
    "print(ab_testing.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "at some point the alberta covid-19 website will probably change. I've included some functions to help with figuring out what's going on. If it's just minor changes, such as changing the id name of a html block, then I'm starting to add in the functionality to pass in an update dictionary to update my scraper searches. Below are the utility functions built into the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.print_html_class_ids\" class=\"doc_header\"><code>albertaC19.print_html_class_ids</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.print_html_class_ids</code>(**`html_class_attr`**:`str`=*`'level2'`*, **`print_self`**:`bool`=*`True`*)\n",
       "\n",
       "websites change so use this if you need to figure out what to use to update the `html_update_class_ids`\n",
       "dictionary. Right now the covid alberta site is using `level 2` as one of the id attributes for\n",
       "the tabs containing the covid data. The scrapers in this are searching for the `id` string.\n",
       "\n",
       "this function will find all `div` `class` sections in the soup with\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    html_class_attr:str\n",
       "        will search through the soup find all class names that include this attribute string\n",
       "    print_self:bool\n",
       "        will also print the current python class parameters `html_class_ids` which are used\n",
       "        by the scraper functions to find the data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.print_html_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'highlights', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'cases', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'characteristics', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'severe-outcomes', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'geospatial', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'laboratory-testing', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'data-export', 'class': ['section', 'level2', 'unnumbered']}\n",
      "{'id': 'data-notes', 'class': ['section', 'level2', 'unnumbered']}\n",
      "Here are the class id's we are currently using for this scraper:\n",
      "{'totals': 'cases', 'regions': 'geospatial', 'testing': 'laboratory-testing'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the output:\n",
    "abscraper.print_html_class_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"albertaC19.update_html_ids\" class=\"doc_header\"><code>albertaC19.update_html_ids</code><a href=\"__main__.py#L61\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>albertaC19.update_html_ids</code>(**`html_update_ids`**:`dict`=*`None`*)\n",
       "\n",
       "update the ids used to search for the data in the alberta covid html\n",
       "current keys are `totals`, `regions`, `testing`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(albertaC19.update_html_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it a dictionary with the keys `totals`, `regions`, and/or `testing` to update the id tag for that scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_webscraper.ipynb.\n",
      "Converted 01_analyses.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
